{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10th exercise: <font color=\"#C70039\">Interpretable Machine Learning by means of Shapley Values</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Date:   04.08.2025\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "<img src=\"https://shap.readthedocs.io/en/latest/_images/example_notebooks_overviews_An_introduction_to_explainable_AI_with_Shapley_values_13_0.png\" style=\"float: center;\" width=\"600\">\n",
    "\n",
    "---------------------------------\n",
    "**GENERAL NOTE 1**: \n",
    "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole. \n",
    "\n",
    "**GENERAL NOTE 2**: \n",
    "* Please, when commenting source code, just use English language only. \n",
    "* When describing an observation please use English language, too.\n",
    "* This applies to all exercises throughout this course.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"ce33ff\">DESCRIPTION</font>:\n",
    "\n",
    "Before using Shapley values to explain complicated models, it is helpful to understand how they work for simple models.\n",
    "\n",
    "In this respect the example in this notebook computes a model for the abalone data set (downloaded from UCI) and uses its outputs for explanation of feature importance using the SHAP explainer. In addition, several different visualization techniques (plots) for Shapley values are going to be demonstrated. \n",
    "\n",
    "For a description of the features please refer to <a href=\"https://archive.ics.uci.edu/dataset/1/abalone\">UCI abalone data set</a>.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"FFC300\">TASKS</font>:\n",
    "The tasks that you need to work on within this notebook are always indicated below as bullet points. \n",
    "If a task is more challenging and consists of several steps, this is indicated as well. \n",
    "Make sure you have worked down the task list and commented your doings. \n",
    "This should be done by using markdown.<br> \n",
    "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
    "\n",
    "**YOUR TASKS in this exercise are as follows**:\n",
    "1. import the notebook to Google Colab or use your local machine.\n",
    "2. make sure you specified you name and your matriculation number in the header below my name and date. \n",
    "    * set the date too and remove mine.\n",
    "3. read the entire notebook carefully \n",
    "    * add comments whereever you feel it necessary for better understanding\n",
    "    * run the notebook for the first time. \n",
    "4. Develop a CNN for image classification and adapt the Shapley Value idea to that model. Comment your entire code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1MnrZSd06ED"
   },
   "source": [
    "## Imports\n",
    "Import all necessary utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/abalone/abalone.data\", names=[\"sex\",\"length\",\"diameter\",\"height\", \n",
    "                                                         \"whole weight\",\"shucked weight\", \"viscera weight\",\"shell weight\",\"rings\"])\n",
    "                         \n",
    "#Get features\n",
    "y = data['rings']\n",
    "X = data[[\"sex\",\"length\",\"height\",\"shucked weight\",\"viscera weight\",\"shell weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some necessary preprocessing\n",
    "X.loc[X['sex'] == 'M', 'sex.M'] = 1\n",
    "X.loc[X['sex'] == 'F', 'sex.F'] = 1\n",
    "X.loc[X['sex'] == 'I', 'sex.I'] = 1\n",
    "\n",
    "X = X.drop('sex', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple ML model and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(objective=\"reg:squarederror\") \n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y, y_pred, alpha=0.5, color='b')\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "\n",
    "# Plot the diagonal line for reference\n",
    "max_val = max(np.max(y), np.max(y_pred))\n",
    "min_val = min(np.min(y), np.min(y_pred))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='dashed')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>NOTE:</font>\n",
    "\n",
    "Even though we did not put much love into our model, the results are quite ok. However, please keep in mind, that you should start computing Shapley value if and only if you have optimized your model. You do not have to be a genius to understand that the better the model, the better the Shapley values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, compute the Shaley values\n",
    "\n",
    "These are the two essential lines of code. Pass your model into the SHAP Explainer function. \n",
    "This creates an explainer object. The, use this to calculate SHAP values for every observation in the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "### Visualization section\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waterfall plot for first observation\n",
    "\n",
    "There are 8 Shapley values for each of the 4177 observations in the feature matrix. \n",
    "That is one Shapley value for each feature in your model. You can use the waterfall function to visualise the Shapley values of the first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"ce33ff\">Interpretation:</font>\n",
    "\n",
    "**E[f(x)] = 9.933** gives the average predicted number of rings across all 4177 abalones. \n",
    "**f(x) = 13.043** is the predicted number of rings for this particular abalone. \n",
    "The Shapley values are all the values in between. \n",
    "For example, the shucked weight contributes to the total predicted number by an increase of **1.68**.\n",
    "\n",
    "There is a unique waterfall plot for every observation of an abalone in your dataset. They can all be interpreted in the same way as above. In each case, the Shapley values tell us how the features have contributed to the prediction when compared to the average prediction. Large positive/negative values indicate that the feature had a significant impact on the model’s prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forceplot for first observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"ce33ff\">Interpretation:</font>\n",
    "\n",
    "Another way to visualise these individual feature contributions is using a so-called force plot. \n",
    "Think of it as a condensed waterfall plot. It starts at the same base value of **9.933** and you can see how each feature has contributed to the final prediction of **13.04**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entire/Stacked force plot\n",
    "\n",
    "Waterfall and force plots are great for interpreting individual predictions (see above). To understand how your model makes predictions in general you need to aggregate the Shapley values. One way to do this is by using the so-called stacked-force plot.\n",
    "\n",
    "You can combine multiple force plots together to create a stacked force plot. In this example pass the Shapley values for the first 100 observations in the force plot function. Each individual force plot is now vertical and stacked side by side.\n",
    "\n",
    "Since this plot is interactive, you can change the ordering of the plots and choose which feature contributions to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"ce33ff\">Interpretation:</font>\n",
    "\n",
    "For example, select \"shell weight\" on x-axis and \"shell weight effects\" on the y-axis.\n",
    "Now, from the appearing plot you can see that as shell weight increases the Shapley values also increase. \n",
    "Since this is a function of age, older abalones tend to have heavier shells.\n",
    "\n",
    "This is one way to understand the nature of the relationships captured by the model. \n",
    "Now, you will learn that the so-called beeswarm plot and dependence plots can also be used this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean SHAPLEY values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"ce33ff\">Interpretation:</font>\n",
    "This bar plot tells you which features are most important. \n",
    "For each and every feature, it calculates the mean Shapley value across all observations. \n",
    "\n",
    "Specifically, it takes the mean of the absolute values as it does not want positive and negative values to offset each other. \n",
    "There is one bar for each feature in the data set. You can easils see, that \"shell weight\" has the largest mean Shapley value.\n",
    "\n",
    "Features that have made large positive/negative contributions will show a large mean Shapley value. In other words, these are the features that have had a significant impact on the model’s predictions. Hence, this bar plot can be used in the same manner as a feature importance plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beeswarm plot\n",
    "\n",
    "The so-called beeswarm plot is one of the most useful plots. The beeswarm visualises all of the SHAP values. \n",
    "On the y-axis, the values are grouped by feature. For each group, the colour of the points is determined by the feature value (i.e. higher feature values are more red - see legend )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"ce33ff\">Interpretation:</font>\n",
    "\n",
    "The features in the above plot are ordered by mean SHAP value.\n",
    "\n",
    "E.g., for \"shell weight\", you will notice, that SHAP values increase when the feature value increases. \n",
    "Remember, you saw a similar relationship in the stacked force plot. It tells you that larger values for \"shell weight\" will lead to a higher predicted number of label value \"rings\".\n",
    "\n",
    "You may also notice, that the feature \"shucked weight\" shows the opposite relationship. \n",
    "Looking at the beeswarm plot, we can see that larger values for this feature are associated with smaller SHAP values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Heatmap\n",
    "\n",
    "Passing the matrix of 200 Shapley values to the heatmap plot function creates a plot with the instances on the x-axis, the model features on the y-axis, and the Shapley values encoded on a color scale. By default, the samples are ordered based on a hierarchical clustering by their explanation similarity. \n",
    "This results in samples that have the same model output for the same reason getting grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.plots.heatmap(shap_values[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"ce33ff\">Interpretation:</font>\n",
    "\n",
    "The output of the model is shown above the heatmap matrix (centered around the explaination’s average value), and the global importance of each feature shown as a bar plot on the right hand side of the plot (by default this is the average measure of overall importance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependence plots of the Shapley values \n",
    "\n",
    "A dependence plot is a scatter plot of the Shapley value vs. the feature value for one single feature. \n",
    "They are particularly useful if the feature has got a non-linear relationship with the label.\n",
    "\n",
    "##### Plot 1: shell weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:,\"shell weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"ce33ff\">Interpretation:</font>\n",
    "\n",
    "For example, take the dependence plot for \"shell weight\" feature. \n",
    "Looking at the beeswarm plot we may have assumed that the Shapley values increase linearly with the feature value. \n",
    "However, this dependency plot shows you that the relationship is not perfectly linear. That is very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot 2: shucked weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:,\"shucked weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"ce33ff\">Interpretation:</font>\n",
    "The dependency plot for \"shucked weight\" (i.e the weight of the abalone meat). \n",
    "Using this plot you can see and confirm the relationship you saw in the beeswarm plot already. \n",
    "The SHAP values do decrease as shucked weight increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot 3: shell weight & shucked weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values[:,\"shell weight\"],\n",
    "                   color=shap_values[:,\"shucked weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"ce33ff\">Interpretation:</font>\n",
    "This plot can be used to visualise interactions between features. BUT be cautious! \n",
    "In your case, the plot is a result of the correlation between the two features.\n",
    "\n",
    "Intuitively, this relationship seems strange. \n",
    "\n",
    "Wouldn’t you expect an older abalone to be larger and having more meat? \n",
    "\n",
    "This is, in fact, a result of an interaction between \"shell weight\" and \"shucked weight\". \n",
    "You could not see it in the dependence plot due to the correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lime_image.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
